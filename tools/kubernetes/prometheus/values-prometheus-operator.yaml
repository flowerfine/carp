# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## 参考 https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

## Create default rules for monitoring the cluster
defaultRules:
  create: false

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
alertmanager:
  enabled: true
  ## Configuration for Alertmanager service
  service:
    ## Port for Alertmanager Service to listen on
    port: 9093
    ## To be used with a proxy extraContainer port
    targetPort: 9093
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30903
    ## Service type
    type: NodePort

## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
## Grafana isn't managed by prometheus
grafana:
  enabled: false

## Flag to disable all the kubernetes component scrapers
kubernetesServiceMonitors:
  enabled: false

## Component scraping the kube api server
kubeApiServer:
  enabled: false

## Component scraping the kubelet and kubelet-hosted cAdvisor
kubelet:
  enabled: false

## Component scraping the kube controller manager
kubeControllerManager:
  enabled: false

## Component scraping coreDns. Use either this or kubeDns
coreDns:
  enabled: false

## Component scraping kubeDns. Use either this or coreDns
kubeDns:
  enabled: false

## Component scraping etcd
kubeEtcd:
  enabled: false

## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false

## Component scraping kube proxy
kubeProxy:
  enabled: false

## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: false

## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: false


## Manages Prometheus and Alertmanager components
prometheusOperator:
  enabled: true
  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
  namespaces:
    releaseNamespace: false
  kubeletService:
    ## If true, the operator will create and maintain a service for scraping kubelets
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
    enabled: false
  ## Create a servicemonitor for the operator
  serviceMonitor:
    ## If true, create a serviceMonitor for prometheus operator
    selfMonitor: false
  ## Resource limits & requests
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 100m
      memory: 100Mi

## Deploy a Prometheus instance
prometheus:
  enabled: true
  ## Configuration for Prometheus service
  service:
    ## Port for Prometheus Service to listen on
    port: 9090
    ## To be used with a proxy extraContainer port
    targetPort: 9090
    ## Port for Prometheus Reloader to listen on
    reloaderWebPort: 8080
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30090
    ## Service type
    type: NodePort

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  prometheusSpec:
    ## ServiceMonitors to be selected for target discovery.
    ## If {}, select all ServiceMonitors
    serviceMonitorSelector: {}
      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
#      matchLabels:
#        group: flink-jobs
#        platform: scaleph

    ## PodMonitors to be selected for target discovery.
    ## If {}, select all PodMonitors
    podMonitorSelector: {}
      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
#      matchLabels:
#        group: flink-jobs
#        platform: scaleph

    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
    scrapeInterval: "2s"
    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ruleSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    serviceMonitorSelectorNilUsesHelmValues: true
    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    podMonitorSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the probes created
    probeSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the scrapeConfigs created
    scrapeConfigSelectorNilUsesHelmValues: false
    ## How long to retain metrics
    retention: 1d
    ## Maximum size of metrics
    retentionSize: ""
    ## Resource limits & requests
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

  additionalServiceMonitors:
    ## Name of the ServiceMonitor to create
    - name: flink-kubernetes-operator-job-metrics
      ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
      ## the chart
      additionalLabels:
        group: flink-kubernetes-operator-job-metrics
        platform: scaleph
      ## labels to transfer from the kubernetes pods to the target
      ## todo 后续在 scaleph 可以增加更多的 labels 到 flink pods 中，通过这种方式传到 prometheus 和 grafana 中去
      podTargetLabels:
        - app
        - component
      ## Label selector for services to which this ServiceMonitor applies
      selector:
        matchLabels:
          app: flink-kubernetes-operator-job-metrics
          component: metrics
          platform: scaleph
      ## Endpoints of the selected service to be monitored
      endpoints:
      ## Name of the endpoint's service port
        - port: prom-metrics
          interval: 10s
      ## HTTP path to scrape for metrics
      # path: /metrics


  additionalPodMonitors: []
    ## Name of the PodMonitor to create
    ##
    # - name: ""

    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
    ## the chart
    ##
    # additionalLabels: {}

    ## Pod label for use in assembling a job name of the form <label value>-<port>
    ## If no label is specified, the pod endpoint name is used.
    ##
    # jobLabel: ""

    ## Label selector for pods to which this PodMonitor applies
    ##
    # selector: {}

    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
    ##
    # podTargetLabels: {}

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    # sampleLimit: 0

    ## Namespaces from which pods are selected
    ##
    # namespaceSelector:
    ## Match any namespace
    ##
    # any: false

    ## Explicit list of namespace names to select
    ##
    # matchNames: []

    ## Endpoints of the selected pods to be monitored
    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
    ##
    # podMetricsEndpoints: []